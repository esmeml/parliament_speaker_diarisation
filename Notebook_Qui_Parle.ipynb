{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOzL1+hnH+0lnvwlL5VY81e"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tpxI9LWuWR2m","executionInfo":{"status":"ok","timestamp":1769607208730,"user_tz":-60,"elapsed":1292,"user":{"displayName":"Petit Chevalier_Bleu","userId":"13076775108994109078"}},"outputId":"328286c3-6431-48ac-e0d6-9cd748070e63"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'parliament_speaker_diarisation'...\n","remote: Enumerating objects: 17, done.\u001b[K\n","remote: Counting objects: 100% (17/17), done.\u001b[K\n","remote: Compressing objects: 100% (15/15), done.\u001b[K\n","remote: Total 17 (delta 3), reused 9 (delta 2), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (17/17), 1.25 MiB | 17.08 MiB/s, done.\n","Resolving deltas: 100% (3/3), done.\n","D√©p√¥t parliament_speaker_diarisation clon√© avec succ√®s !\n","/content/parliament_speaker_diarisation\n"]}],"source":["from google.colab import userdata\n","import os\n","\n","# 1. R√©cup√©ration des secrets\n","try:\n","    token = userdata.get('GITHUB_TOKEN')\n","except ImportError:\n","    print(\"Erreur: Assurez-vous d'avoir ajout√© le secret GITHUB_TOKEN dans la barre lat√©rale.\")\n","    token = input(\"Entrez votre token manuellement: \")\n","\n","# 2. Configuration des variables\n","username = \"Thorbjornen\"\n","repo_owner = \"esmeml\" # Le compte qui poss√®de le repo\n","repo_name = \"parliament_speaker_diarisation\"\n","user_email = \"niels.groeninck4@gmail.com\"\n","\n","# 3. Configuration de l'identit√© Git (n√©cessaire pour commit)\n","!git config --global user.email \"{user_email}\"\n","!git config --global user.name \"{username}\"\n","\n","# 4. Clonage du d√©p√¥t avec authentification\n","# La syntaxe est : https://<token>@github.com/<owner>/<repo>.git\n","clone_url = f\"https://{token}@github.com/{repo_owner}/{repo_name}.git\"\n","\n","# On clone uniquement si le dossier n'existe pas d√©j√†\n","if not os.path.exists(repo_name):\n","    !git clone {clone_url}\n","    print(f\"D√©p√¥t {repo_name} clon√© avec succ√®s !\")\n","else:\n","    print(f\"Le dossier {repo_name} existe d√©j√†.\")\n","\n","# 5. Se d√©placer dans le dossier du d√©p√¥t\n","%cd {repo_name}"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"70adac67","executionInfo":{"status":"ok","timestamp":1769508905493,"user_tz":-60,"elapsed":971,"user":{"displayName":"Petit Chevalier_Bleu","userId":"13076775108994109078"}},"outputId":"82a69f24-5ce5-49be-8069-acda93b89866"},"source":["from google.colab import drive\n","import shutil\n","import os\n","\n","# acc√®s au drive\n","drive.mount('/content/drive')\n","\n","nom_du_notebook = \"Notebook_Qui_Parle.ipynb\"\n","\n","chemin_source = f\"/content/drive/My Drive/Colab Notebooks/{nom_du_notebook}\"\n","\n","# Nom du dossier Git (celui clon√© pr√©c√©demment)\n","nom_dossier_git = \"parliament_speaker_diarisation\"\n","# -------------------------------\n","\n","# 2. V√©rification que le fichier source existe\n","if not os.path.exists(chemin_source):\n","    print(f\"ERREUR : Le fichier n'a pas √©t√© trouv√© √† l'endroit : {chemin_source}\")\n","    print(\"V√©rifiez le nom du fichier ou s'il est dans un sous-dossier de votre Drive.\")\n","else:\n","    destination_dir = f\"/content/{nom_dossier_git}\"\n","    os.makedirs(destination_dir, exist_ok=True) # S'assurer que le dossier de destination existe\n","\n","    # 3. Copie du notebook vers le dossier Git\n","    destination = os.path.join(destination_dir, nom_du_notebook)\n","    shutil.copy(chemin_source, destination)\n","    print(f\"Succ√®s : Le notebook a √©t√© copi√© dans le dossier Git !\")\n","\n","    # 4. Envoi vers GitHub\n","    # Changement du repertoire de travail\n","    os.chdir(destination_dir)\n","\n","    print(\"Envoi vers GitHub en cours...\")\n","    !git add \"{nom_du_notebook}\"  # On ajoute sp√©cifiquement le notebook\n","    !git commit -m \"Mise √† jour du notebook de d√©ploiement\"\n","    !git push origin main # Ou 'master' selon votre branche\n","\n","    print(\"Termin√© ! Le notebook est maintenant sur le repo.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","ERREUR : Le dossier '/content/parliament_speaker_diarisation' n'est pas un d√©p√¥t Git valide.\n","Veuillez ex√©cuter la premi√®re cellule du notebook pour cloner le d√©p√¥t avant de continuer.\n"]}]},{"cell_type":"code","source":["# Installation des librairies n√©cessaires pour le traitement audio et le XML\n","!pip install librosa soundfile lxml tensorflow\n","\n","import os\n","import numpy as np\n","import pandas as pd\n","import librosa\n","import soundfile as sf\n","import xml.etree.ElementTree as ET\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from google.colab import drive\n","\n","# 1. Connexion au Google Drive\n","drive.mount('/content/drive')\n","print(\"Google Drive connect√©.\")"],"metadata":{"id":"ueS_2IPzYNK3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["AUDIO_FILE_NAME = '4_decembre.wav'\n","XML_FILE_NAME = '4_decembre.xml'\n","\n","# Chemins (Racine du Drive)\n","BASE_DIR = '/content/drive/MyDrive'\n","AUDIO_PATH = os.path.join(BASE_DIR, AUDIO_FILE_NAME)\n","XML_PATH = os.path.join(BASE_DIR, XML_FILE_NAME)\n","\n","# Dossier o√π seront cr√©√©s les petits segments audio\n","OUTPUT_DATASET_DIR = os.path.join(BASE_DIR, 'Dataset_Assemblee_CTC')\n","CSV_MANIFEST_PATH = os.path.join(OUTPUT_DATASET_DIR, 'metadata.csv')\n","\n","# Param√®tres Audio pour CTC\n","TARGET_SR = 16000  # Standard pour la reconnaissance vocale (16kHz)\n","\n","# Cr√©ation du dossier de sortie s'il n'existe pas\n","os.makedirs(OUTPUT_DATASET_DIR, exist_ok=True)\n","\n","print(f\"Dossier de travail : {OUTPUT_DATASET_DIR}\")\n","if not os.path.exists(AUDIO_PATH):\n","    print(f\"ERREUR : Le fichier audio '{AUDIO_FILE_NAME}' est introuvable √† la racine du Drive !\")\n","else:\n","    print(f\"Fichier audio d√©tect√© : {AUDIO_PATH}\")"],"metadata":{"id":"hBWt5Z9gYS0t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import xml.etree.ElementTree as ET\n","import json\n","import os\n","import re\n","\n","# Config\n","XML_FILE = '/content/drive/MyDrive/4_decembre.xml'\n","JSON_OUTPUT = '/content/drive/MyDrive/segments_clean.json'\n","\n","def xml_to_json_segments(xml_path):\n","    print(f\"Lecture du fichier : {xml_path}\")\n","\n","    with open(xml_path, 'r', encoding='utf-8') as f:\n","        content = f.read()\n","\n","    # On retire les namespaces\n","    content = re.sub(r' xmlns=\"[^\"]+\"', '', content)\n","\n","    root = ET.fromstring(content)\n","\n","    raw_segments = []\n","\n","    # On utilise .iter() pour aller chercher l'info partout\n","    all_paras = list(root.iter('paragraphe'))\n","    print(f\"Nombre de paragraphes trouv√©s : {len(all_paras)}\")\n","\n","    for para in all_paras:\n","        texte_node = para.find('texte')\n","\n","        if texte_node is not None and 'stime' in texte_node.attrib:\n","            try:\n","                start_time = float(texte_node.attrib['stime'])\n","\n","                # 1. Extraction brute de tout le texte (y compris dans les balises enfants)\n","                raw_text = \"\".join(texte_node.itertext())\n","\n","                # 2. Enlever les \\n\n","                clean_text = \" \".join(raw_text.split())\n","\n","                # 3. Chercher le locuteur\n","                speaker = \"Inconnu\"\n","                orateurs = para.find('orateurs')\n","                if orateurs is not None:\n","                    orateur = orateurs.find('orateur')\n","                    if orateur is not None:\n","                        nom = orateur.find('nom')\n","                        if nom is not None:\n","                            speaker = nom.text\n","\n","                if clean_text:\n","                    raw_segments.append({\n","                        'start': start_time,\n","                        'speaker': speaker,\n","                        'text': clean_text\n","                    })\n","\n","            except ValueError:\n","                continue\n","\n","    # Tri et calcul des dur√©es\n","    raw_segments.sort(key=lambda x: x['start'])\n","\n","    final_dataset = []\n","\n","    for i in range(len(raw_segments) - 1):\n","        curr = raw_segments[i]\n","        nxt = raw_segments[i+1]\n","\n","        duration = nxt['start'] - curr['start']\n","\n","        # Filtres de qualit√©\n","        # On garde si c'est raisonnable (entre 0.5s et 60s)\n","        # On augmente un peu la tol√©rance car les applaudissements peuvent √™tre longs\n","        if 0.5 <= duration <= 60.0:\n","            curr['end'] = nxt['start']\n","            curr['duration'] = round(duration, 3)\n","            final_dataset.append(curr)\n","\n","    # Sauvegarde\n","    with open(JSON_OUTPUT, 'w', encoding='utf-8') as f:\n","        json.dump(final_dataset, f, indent=4, ensure_ascii=False)\n","\n","    print(f\"Succ√®s ! {len(final_dataset)} segments sauvegard√©s dans :\")\n","    print(f\"   -> {JSON_OUTPUT}\")\n","\n","    # V√©rification visuelle\n","    if len(final_dataset) > 0:\n","        print(\"\\nExemple corrig√© (regardez le champ 'text') :\")\n","        # On cherche un segment qui contient une parenth√®se pour v√©rifier\n","        for seg in final_dataset:\n","            if '(' in seg['text']:\n","                print(json.dumps(seg, indent=4, ensure_ascii=False))\n","                break\n","\n","# Lancer la correction\n","xml_to_json_segments(XML_FILE)"],"metadata":{"id":"q2HnkqmvYeb1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","import librosa\n","import soundfile as sf\n","import pandas as pd\n","from tqdm import tqdm\n","\n","# Config\n","# Le fichier JSON cr√©√© pr√©c√©demment\n","JSON_FILE = '/content/drive/MyDrive/segments_clean.json'\n","# Fichier audio source\n","AUDIO_FILE = '/content/drive/MyDrive/4_decembre.wav'\n","# Le dossier pour petits fichiers wav (segments)\n","OUTPUT_DIR = '/content/drive/MyDrive/Dataset_Assemblee_CTC'\n","\n","# Cr√©ation du dossier si n√©cessaire\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","\n","def create_audio_dataset_from_json():\n","    \"\"\"\n","    Lit le JSON, charge l'audio, d√©coupe et sauvegarde les segments.\n","    G√©n√®re aussi le CSV final pour l'entra√Ænement.\n","    \"\"\"\n","\n","    # 1. V√©rification du fichier JSON\n","    if not os.path.exists(JSON_FILE):\n","        print(f\"ERREUR : Le fichier JSON {JSON_FILE} n'existe pas !\")\n","        print(\"   -> Avez-vous bien lanc√© la fonction xml_to_json_segments() avant ?\")\n","        return\n","\n","    # 2. Chargement des segments depuis le JSON\n","    print(f\"Chargement du JSON : {JSON_FILE}\")\n","    with open(JSON_FILE, 'r', encoding='utf-8') as f:\n","        segments = json.load(f)\n","    print(f\"{len(segments)} segments charg√©s √† traiter.\")\n","\n","    # 3. Chargement de l'audio complet\n","    print(f\"Chargement du fichier audio complet en m√©moire (16kHz)... Cela peut prendre 1 √† 2 minutes.\")\n","    # On force le chargement en 16000 Hz car c'est le standard pour CTC/ASR\n","    try:\n","        y, sr = librosa.load(AUDIO_FILE, sr=16000)\n","        print(f\"Audio charg√© ! Dur√©e totale : {len(y)/sr:.2f} secondes.\")\n","    except Exception as e:\n","        print(f\"ERREUR lors du chargement de l'audio : {e}\")\n","        return\n","\n","    csv_data = []\n","\n","    print(\"D√©coupage et sauvegarde des segments audio...\")\n","    # tqdm pour voir la progression\n","    for i, seg in enumerate(tqdm(segments)):\n","        try:\n","            # Calcul des index de d√©but et fin\n","            start_sample = int(seg['start'] * sr)\n","            end_sample = int(seg['end'] * sr)\n","\n","            # Extraction du bout d'audio\n","            # V√©rification pour ne pas d√©passer la taille du fichier\n","            if end_sample > len(y):\n","                end_sample = len(y)\n","\n","            clip = y[start_sample:end_sample]\n","\n","            # Sauvegarde du petit fichier wav\n","            filename = f\"segment_{i:05d}.wav\"\n","            path = os.path.join(OUTPUT_DIR, filename)\n","            sf.write(path, clip, sr)\n","\n","            # On ajoute les infos pour le CSV\n","            csv_data.append({\n","                \"filename\": filename,\n","                \"text\": seg['text'],\n","                \"speaker\": seg['speaker'],\n","                \"duration\": seg['duration']\n","            })\n","\n","        except Exception as e:\n","            print(f\"Erreur sur le segment {i} : {e}\")\n","            continue\n","\n","    # 4. Cr√©ation du fichier Metadata (CSV)\n","    df = pd.DataFrame(csv_data)\n","    csv_path = os.path.join(OUTPUT_DIR, 'metadata.csv')\n","    df.to_csv(csv_path, index=False)\n","\n","    print(f\"\\nTRAITEMENT TERMIN√â\")\n","    print(f\"   -> {len(df)} fichiers audio cr√©√©s dans : {OUTPUT_DATASET_DIR}\")\n","    print(f\"   -> Fichier index (CSV) cr√©√© : {csv_path}\")\n","    print(\"\\n   Aper√ßu du CSV :\")\n","    print(df.head())\n","\n","# Lancement\n","create_audio_dataset_from_json()"],"metadata":{"id":"tQ2O3DBvZAwd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Dans la cellule pr√©c√©dente on a eu un probl√®me de d√©callage"],"metadata":{"id":"8x5gOZyIZB-q"}},{"cell_type":"markdown","source":["Connexion au drive"],"metadata":{"id":"R7AzziTqaBg9"}},{"cell_type":"code","source":["from google.colab import drive\n","\n","# 1. Connexion au Google Drive\n","drive.mount('/content/drive')\n","print(\"Google Drive connect√©.\")"],"metadata":{"id":"g2nWpb-VZxju"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Whisper"],"metadata":{"id":"E4IADGmTbsZB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","import os\n","import soundfile as sf\n","import librosa\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","\n","# Config\n","# Fichiers\n","JSON_FILE = '/content/drive/MyDrive/segments_perfect.json'\n","AUDIO_FILE = '/content/drive/MyDrive/4_decembre.wav'\n","\n","# Dossier de sortie\n","OUTPUT_DIR = '/content/drive/MyDrive/Dataset_Assemblee_Final'\n","WAVS_DIR = os.path.join(OUTPUT_DIR, 'wavs')\n","\n","# Param√®tres audio cibles pour l'entra√Ænement\n","TARGET_SR = 16000 # 16kHz est le standard\n","Target_CHANNELS = 1 # Mono\n","\n","# Cr√©ation de l'arborescence\n","os.makedirs(WAVS_DIR, exist_ok=True)\n","\n","def cut_and_save_dataset():\n","    # 1. V√©rifications\n","    if not os.path.exists(JSON_FILE):\n","        print(f\"ERREUR : Le JSON {JSON_FILE} n'existe pas.\")\n","        return\n","\n","    print(f\"Lecture du JSON : {JSON_FILE}\")\n","    with open(JSON_FILE, 'r', encoding='utf-8') as f:\n","        segments = json.load(f)\n","    print(f\"{len(segments)} segments √† traiter.\")\n","\n","    # 2. Analyse du fichier audio source\n","    print(\"Analyse du fichier audio source...\")\n","    try:\n","        info = sf.info(AUDIO_FILE)\n","        native_sr = info.samplerate\n","        print(f\"   -> Source : {info.duration/3600:.2f} heures @ {native_sr}Hz\")\n","    except Exception as e:\n","        print(f\"Erreur lecture audio : {e}\")\n","        return\n","\n","    csv_data = []\n","\n","    print(\"D√©coupage, R√©√©chantillonnage (16k) et Sauvegarde...\")\n","\n","    # Barre de progression\n","    for i, seg in enumerate(tqdm(segments)):\n","        try:\n","            # Calcul des bornes\n","            start_sec = seg['start']\n","            end_sec = seg['end']\n","            duration = end_sec - start_sec\n","\n","            # S√©curit√© : on ignore les segments vides ou aberrants\n","            if duration < 0.1: continue\n","\n","            # Conversion en frames (√©chantillons) pour soundfile\n","            start_frame = int(start_sec * native_sr)\n","            frames_to_read = int(duration * native_sr)\n","\n","            # Lecture stream\n","            # On lit seulement le petit morceau n√©cessaire\n","            audio_clip, _ = sf.read(AUDIO_FILE, start=start_frame, frames=frames_to_read, always_2d=True)\n","\n","            # Traitement audio\n","            # 1. Conversion St√©r√©o -> Mono (Moyenne des canaux)\n","            if audio_clip.shape[1] > 1:\n","                audio_clip = np.mean(audio_clip, axis=1)\n","            else:\n","                audio_clip = audio_clip.flatten()\n","\n","            # 2. R√©√©chantillonnage vers 16kHz (si n√©cessaire)\n","            if native_sr != TARGET_SR:\n","                # librosa.resample est efficace sur des petits clips\n","                audio_clip = librosa.resample(audio_clip, orig_sr=native_sr, target_sr=TARGET_SR)\n","\n","            # Sauvegarde\n","            filename = f\"segment_{i:05d}.wav\"\n","            file_path = os.path.join(WAVS_DIR, filename)\n","\n","            # On √©crit le fichier propre (16k, mono)\n","            sf.write(file_path, audio_clip, TARGET_SR)\n","\n","            # Metadata\n","            # On ajoute les infos pour le CSV final\n","            csv_data.append({\n","                \"filename\": filename,          # Nom du fichier (relatif)\n","                \"text\": seg['text'],           # Le texte de Whisper\n","                \"speaker\": seg['speaker'],     # L'orateur du XML\n","                \"duration\": round(len(audio_clip)/TARGET_SR, 2) # Dur√©e r√©elle\n","            })\n","\n","        except Exception as e:\n","            print(f\"Erreur sur le segment {i} : {e}\")\n","            continue\n","\n","    # 3. Cr√©ation du fichier Index (CSV)\n","    df = pd.DataFrame(csv_data)\n","    csv_path = os.path.join(OUTPUT_DIR, 'metadata.csv')\n","    df.to_csv(csv_path, index=False)\n","\n","    print(f\"\\nDATASET FINAL TERMIN√â !\")\n","    print(f\"Dossier : {OUTPUT_DIR}\")\n","    print(f\"CSV Index : {csv_path}\")\n","    print(f\"Nombre d'exemples : {len(df)}\")\n","    print(\"\\nAper√ßu des donn√©es :\")\n","    print(df.head())\n","\n","# Lancement\n","cut_and_save_dataset()"],"metadata":{"id":"wO196-I7aG5o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Envoie sur le repo"],"metadata":{"id":"C1ggJrxmdkSr"}},{"cell_type":"code","source":["from google.colab import drive\n","import shutil\n","import os\n","\n","# acc√®s au drive\n","drive.mount('/content/drive')\n","\n","nom_du_notebook = \"Notebook_Qui_Parle.ipynb\"\n","\n","chemin_source = f\"/content/drive/My Drive/Colab Notebooks/{nom_du_notebook}\"\n","\n","# Nom du dossier Git (celui clon√© pr√©c√©demment)\n","nom_dossier_git = \"parliament_speaker_diarisation\"\n","# -------------------------------\n","\n","# 2. V√©rification que le fichier source existe\n","if not os.path.exists(chemin_source):\n","    print(f\"ERREUR : Le fichier n'a pas √©t√© trouv√© √† l'endroit : {chemin_source}\")\n","    print(\"V√©rifiez le nom du fichier ou s'il est dans un sous-dossier de votre Drive.\")\n","else:\n","    # 3. Copie du notebook vers le dossier Git\n","    destination = f\"/content/{nom_dossier_git}/{nom_du_notebook}\"\n","    shutil.copy(chemin_source, destination)\n","    print(f\"Succ√®s : Le notebook a √©t√© copi√© dans le dossier Git !\")\n","\n","    # 4. Envoi vers GitHub\n","    # Changement du repertoire de travail\n","    os.chdir(f\"/content/{nom_dossier_git}\")\n","\n","    print(\"Envoi vers GitHub en cours...\")\n","    !git add \"{nom_du_notebook}\"  # On ajoute sp√©cifiquement le notebook\n","    !git commit -m \"Mise √† jour du notebook de d√©ploiement\"\n","    !git push origin main # Ou 'master' selon votre branche\n","\n","    print(\"Termin√© ! Le notebook est maintenant sur le repo.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VPE8GpEDdjtQ","executionInfo":{"status":"ok","timestamp":1768209273251,"user_tz":-60,"elapsed":3764,"user":{"displayName":"Petit Chevalier_Bleu","userId":"13076775108994109078"}},"outputId":"d92c621a-fc73-4326-c60a-5f9ffe0a4843"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Succ√®s : Le notebook a √©t√© copi√© dans le dossier Git !\n","Envoi vers GitHub en cours...\n","[main fdbf76b] Mise √† jour du notebook de d√©ploiement\n"," 1 file changed, 1 insertion(+)\n"," create mode 100644 Notebook_Qui_Parle.ipynb\n","Enumerating objects: 4, done.\n","Counting objects: 100% (4/4), done.\n","Delta compression using up to 2 threads\n","Compressing objects: 100% (3/3), done.\n","Writing objects: 100% (3/3), 6.46 KiB | 6.46 MiB/s, done.\n","Total 3 (delta 1), reused 0 (delta 0), pack-reused 0\n","remote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n","To https://github.com/esmeml/parliament_speaker_diarisation.git\n","   c854166..fdbf76b  main -> main\n","Termin√© ! Le notebook est maintenant sur le repo.\n"]}]},{"cell_type":"markdown","source":["PARTIE CLUSTERING"],"metadata":{"id":"Cmerh2ieGDsE"}},{"cell_type":"code","source":["# Installation des librairies n√©cessaires\n","# On force des versions compatibles pour √©viter les erreurs rouges\n","!pip install \"numpy<2.0\" \"pandas<3.0\" \"pyannote.audio==3.1.1\" soundfile"],"metadata":{"id":"9W1OIoaoGC_u"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"id":"6ae96347","executionInfo":{"status":"error","timestamp":1769607164630,"user_tz":-60,"elapsed":840,"user":{"displayName":"Petit Chevalier_Bleu","userId":"13076775108994109078"}},"outputId":"0ede80c4-5736-4b2f-dd03-e93565ab9227"},"source":["from google.colab import drive\n","import shutil\n","import os\n","\n","# acc√®s au drive\n","drive.mount('/content/drive')\n","\n","nom_du_notebook = \"Notebook_Qui_Parle.ipynb\"\n","\n","chemin_source = f\"/content/drive/My Drive/Colab Notebooks/{nom_du_notebook}\"\n","\n","# Nom du dossier Git (celui clon√© pr√©c√©demment)\n","nom_dossier_git = \"parliament_speaker_diarisation\"\n","# -------------------------------\n","\n","# 2. V√©rification que le fichier source existe\n","if not os.path.exists(chemin_source):\n","    print(f\"ERREUR : Le fichier n'a pas √©t√© trouv√© √† l'endroit : {chemin_source}\")\n","    print(\"V√©rifiez le nom du fichier ou s'il est dans un sous-dossier de votre Drive.\")\n","else:\n","    # 3. Copie du notebook vers le dossier Git\n","    destination = f\"/content/{nom_dossier_git}/{nom_du_notebook}\"\n","    shutil.copy(chemin_source, destination)\n","    print(f\"Succ√®s : Le notebook a √©t√© copi√© dans le dossier Git !\")\n","\n","    # 4. Envoi vers GitHub\n","    # Changement du repertoire de travail\n","    os.chdir(f\"/content/{nom_dossier_git}\")\n","\n","    print(\"Envoi vers GitHub en cours...\")\n","    !git add \"{nom_du_notebook}\"  # On ajoute sp√©cifiquement le notebook\n","    !git commit -m \"Mise √† jour du notebook de d√©ploiement\"\n","    !git push origin main # Ou 'master' selon votre branche\n","\n","    print(\"Termin√© ! Le notebook est maintenant sur le repo.\")"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/parliament_speaker_diarisation/Notebook_Qui_Parle.ipynb'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3474664995.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# 3. Copie du notebook vers le dossier Git\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mdestination\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"/content/{nom_dossier_git}/{nom_du_notebook}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchemin_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Succ√®s : Le notebook a √©t√© copi√© dans le dossier Git !\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m     \u001b[0mcopymode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m                     \u001b[0;31m# macOS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0m_HAS_FCOPYFILE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/parliament_speaker_diarisation/Notebook_Qui_Parle.ipynb'"]}]},{"cell_type":"markdown","source":["Redemarrer session"],"metadata":{"id":"hK4elMgTGP7Z"}},{"cell_type":"code","source":["import sys\n","import types\n","import os\n","import torch\n","import torchaudio\n","import soundfile as sf\n","from google.colab import drive, userdata\n","\n","print(\"1. Application des correctifs syst√®mes...\")\n","\n","# fonctions torchaudio\n","if not hasattr(torchaudio, \"set_audio_backend\"):\n","    torchaudio.set_audio_backend = lambda x: None\n","if not hasattr(torchaudio, \"get_audio_backend\"):\n","    torchaudio.get_audio_backend = lambda: \"soundfile\"\n","if not hasattr(torchaudio, \"list_audio_backends\"):\n","    torchaudio.list_audio_backends = lambda: [\"soundfile\"]\n","\n","# Modules Torchaudio manquants\n","dummy_backend = types.ModuleType(\"torchaudio.backend\")\n","dummy_common = types.ModuleType(\"torchaudio.backend.common\")\n","try:\n","    dummy_common.AudioMetaData = torchaudio.AudioMetaData\n","except AttributeError:\n","    class AudioMetaData:\n","        def __init__(self, sr, nf, nc, bps, enc):\n","            self.sample_rate = sr\n","            self.num_frames = nf\n","            self.num_channels = nc\n","            self.bits_per_sample = bps\n","            self.encoding = enc\n","    dummy_common.AudioMetaData = AudioMetaData\n","sys.modules[\"torchaudio.backend\"] = dummy_backend\n","sys.modules[\"torchaudio.backend.common\"] = dummy_common\n","\n","# On sauvegarde l'original une seule fois pour √©viter la boucle infinie\n","if not hasattr(torch, \"_original_load_backup\"):\n","    torch._original_load_backup = torch.load\n","\n","def permissive_load(*args, **kwargs):\n","    kwargs['weights_only'] = False\n","    return torch._original_load_backup(*args, **kwargs)\n","torch.load = permissive_load\n","\n","print(\"Correctifs appliqu√©s. Le terrain est pr√™t.\")\n","\n","# import de pyannote\n","from pyannote.audio import Pipeline\n","\n","# Connexion Drive\n","if not os.path.exists('/content/drive'):\n","    drive.mount('/content/drive')\n","\n","FILE_PATH = \"/content/drive/MyDrive/Qui_parle/2M_48k.wav\"\n","\n","if not os.path.exists(FILE_PATH):\n","    raise FileNotFoundError(f\"Fichier introuvable : {FILE_PATH}\")\n","\n","print(f\"Fichier trouv√© : {FILE_PATH}\")\n","\n","try:\n","    TOKEN = userdata.get('HF_TOKEN')\n","except:\n","    TOKEN = \"VOTRE_TOKEN_ICI\"\n","\n","MIN_SPEAKERS = 10\n","MAX_SPEAKERS = 15\n","\n","print(\"\\nChargement du pipeline...\")\n","pipeline = Pipeline.from_pretrained(\n","    \"pyannote/speaker-diarization-3.1\",\n","    use_auth_token=TOKEN\n",")\n","pipeline.to(torch.device(\"cuda\"))\n","\n","print(\"Lecture audio manuelle (SoundFile)...\")\n","audio_data, sample_rate = sf.read(FILE_PATH)\n","waveform = torch.tensor(audio_data, dtype=torch.float32)\n","\n","# Correction Mono/St√©r√©o\n","if waveform.dim() == 1:\n","    waveform = waveform.unsqueeze(0)\n","elif waveform.shape[1] < waveform.shape[0]:\n","    waveform = waveform.t()\n","\n","waveform = waveform.to(torch.device(\"cuda\"))\n","\n","print(\"D√©marrage de l'analyse...\")\n","diarization = pipeline(\n","    {\"waveform\": waveform, \"sample_rate\": sample_rate},\n","    min_speakers=MIN_SPEAKERS,\n","    max_speakers=MAX_SPEAKERS\n",")\n","\n","print(\"\\n=== R√âSULTATS ===\")\n","for turn, _, speaker in diarization.itertracks(yield_label=True):\n","    print(f\"Speaker {speaker}: {turn.start:.1f}s -> {turn.end:.1f}s\")\n","\n","\n"],"metadata":{"id":"1qSPYb9AGRHV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Sauvegarde dans Drive"],"metadata":{"id":"mfUAZefvGqwD"}},{"cell_type":"code","source":["# On d√©finit le chemin de sauvegarde sur le Drive\n","OUTPUT_RTTM = \"/content/drive/MyDrive/Qui_parle/resultats_diarisation.rttm\"\n","\n","print(f\"\\nSauvegarde des r√©sultats dans : {OUTPUT_RTTM} ...\")\n","\n","# La fonction magique de Pyannote pour sauvegarder\n","with open(OUTPUT_RTTM, \"w\") as rttm_file:\n","    diarization.write_rttm(rttm_file)\n","\n","print(\"Sauvegarde termin√©e ! En cas de crash, vous pourrez recharger ce fichier.\")"],"metadata":{"id":"LOVQT9uNGnu0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","from google.colab import drive\n","import os\n","\n","# outils pour diarization\n","from pyannote.core import Annotation, Segment\n","\n","# connexion drive\n","if not os.path.exists('/content/drive'):\n","    drive.mount('/content/drive')\n","\n","RTTM_PATH = \"/content/drive/MyDrive/Qui_parle/resultats_diarisation.rttm\"\n","\n","if not os.path.exists(RTTM_PATH):\n","    print(f\"ERREUR : Le fichier n'existe pas : {RTTM_PATH}\")\n","    print(\"Avez-vous bien ex√©cut√© l'√©tape de sauvegarde pr√©c√©dente ?\")\n","else:\n","    print(f\"Fichier trouv√© : {RTTM_PATH}\")\n","\n","    # recharger les donn√©es\n","    def charger_rttm_depuis_drive(chemin_fichier):\n","        \"\"\"Lit un fichier RTTM et recr√©e un objet Annotation pour le graphique\"\"\"\n","        annotation = Annotation()\n","\n","        with open(chemin_fichier, 'r') as f:\n","            for line in f:\n","                parts = line.strip().split()\n","                # Format standard RTTM :\n","                # SPEAKER file 1 start duration <NA> <NA> name <NA>\n","                if len(parts) >= 8 and parts[0] == \"SPEAKER\":\n","                    start = float(parts[3])\n","                    duration = float(parts[4])\n","                    speaker_label = parts[7]\n","\n","                    # On ajoute le segment √† l'annotation\n","                    annotation[Segment(start, start + duration)] = speaker_label\n","\n","        return annotation\n","\n","    # fonction de visualisation\n","    def visualiser_diarisation(diarization):\n","        # Calcul dynamique de la hauteur : 1 pouce par 3 speakers, minimum 6\n","        nb_speakers = len(diarization.labels())\n","        hauteur = max(6, nb_speakers * 0.5)\n","\n","        fig, ax = plt.subplots(figsize=(20, hauteur))\n","\n","        # On r√©cup√®re tous les locuteurs uniques pour leur donner une couleur/hauteur\n","        speakers = sorted(diarization.labels()) # Tri√© pour √™tre plus propre\n","        speaker_map = {label: i for i, label in enumerate(speakers)}\n","\n","        # On trace chaque segment\n","        for turn, _, speaker in diarization.itertracks(yield_label=True):\n","            idx = speaker_map[speaker]\n","            # On dessine une barre horizontale\n","            ax.hlines(idx, turn.start, turn.end, linewidth=30, colors=f\"C{idx}\")\n","\n","        # Mise en forme\n","        ax.set_yticks(list(speaker_map.values()))\n","        ax.set_yticklabels(list(speaker_map.keys()))\n","        ax.set_xlabel(\"Temps (secondes)\")\n","        ax.set_title(\"Timeline des Locuteurs (Charg√©e depuis le Drive)\")\n","        ax.grid(True, axis='x', alpha=0.3)\n","\n","        plt.tight_layout()\n","        plt.show()\n","\n","    # Ex√©cution\n","    print(\"Chargement des donn√©es...\")\n","    # C'est ici que la magie op√®re : on recr√©e la variable 'diarization' depuis le fichier texte\n","    diarization_rechargee = charger_rttm_depuis_drive(RTTM_PATH)\n","\n","    print(\"G√©n√©ration du graphique...\")\n","    visualiser_diarisation(diarization_rechargee)"],"metadata":{"id":"_udaBKo2G1L-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["try:\n","    import matplotlib.pyplot as plt\n","except ImportError:\n","    !pip install -qq matplotlib\n","    import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# fonction de vizualisation\n","def visualiser_diarisation(diarization):\n","    # Vous pouvez augmenter (20, 5) si vous avez beaucoup de locuteurs\n","    fig, ax = plt.subplots(figsize=(20, 6))\n","\n","    # On r√©cup√®re tous les locuteurs uniques\n","    speakers = diarization.labels()\n","    speaker_map = {label: i for i, label in enumerate(speakers)}\n","\n","    # On trace chaque segment\n","    for turn, _, speaker in diarization.itertracks(yield_label=True):\n","        idx = speaker_map[speaker]\n","        # On dessine une barre horizontale\n","        # 'C{idx}' utilise les couleurs par d√©faut de Matplotlib (C0, C1, etc.)\n","        ax.hlines(idx, turn.start, turn.end, linewidth=30, colors=f\"C{idx}\")\n","\n","    # Mise en forme\n","    ax.set_yticks(list(speaker_map.values()))\n","    ax.set_yticklabels(list(speaker_map.keys()))\n","    ax.set_xlabel(\"Temps (secondes)\")\n","    ax.set_title(\"Timeline des Locuteurs (Qui parle quand ?)\")\n","    ax.grid(True, axis='x', alpha=0.3) # Grille verticale pour se rep√©rer\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","# Lancement de la visualisation\n","# (Cela suppose que la variable 'diarization' existe d√©j√† suite √† l'√©tape pr√©c√©dente)\n","print(\"G√©n√©ration du graphique...\")\n","visualiser_diarisation(diarization)"],"metadata":{"id":"vQ5XGXazHUFf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["import de whisper"],"metadata":{"id":"UxpY2z4DHjqg"}},{"cell_type":"code","source":["!pip install -qq git+https://github.com/openai/whisper.git soundfile torch torchaudio"],"metadata":{"id":"L8vAVs4rHl-r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Diarisation whisper"],"metadata":{"id":"PR6AGeeSHvbX"}},{"cell_type":"code","source":["import torch\n","import torchaudio.transforms as T\n","import soundfile as sf\n","import whisper\n","import os\n","from google.colab import drive\n","\n","# 1. CONNEXION AU DRIVE\n","if not os.path.exists('/content/drive'):\n","    drive.mount('/content/drive')\n","\n","# 2. CHARGEMENT DU FICHIER AUDIO (Pour la d√©coupe)\n","print(\"Lecture du fichier audio...\")\n","FILE_PATH = \"/content/drive/MyDrive/Qui_parle/2M_48k.wav\"\n","RTTM_PATH = \"/content/drive/MyDrive/Qui_parle/resultats_diarisation.rttm\" # <-- Le fichier de secours\n","\n","if not os.path.exists(FILE_PATH):\n","    print(\"ERREUR : Audio introuvable.\")\n","elif not os.path.exists(RTTM_PATH):\n","    print(f\"ERREUR : Le fichier de sauvegarde {RTTM_PATH} est introuvable.\")\n","    print(\"La sauvegarde n'a peut-√™tre pas eu le temps de se finir avant le crash ?\")\n","else:\n","    # Lecture Audio\n","    audio_data, sample_rate = sf.read(FILE_PATH)\n","    waveform = torch.tensor(audio_data, dtype=torch.float32)\n","    if waveform.dim() == 1: waveform = waveform.unsqueeze(0)\n","    elif waveform.shape[1] < waveform.shape[0]: waveform = waveform.t()\n","    waveform = waveform.to(\"cuda\")\n","\n","    print(f\"Audio charg√© ({sample_rate} Hz).\")\n","    print(f\"Fichier de diarisation trouv√© ({RTTM_PATH}).\")\n","\n","    # 3. VOS NOMS\n","    NOMS_REELS = {\n","        \"SPEAKER_00\": \"Mme Estelle Youssouffa\",\n","        \"SPEAKER_01\": \"Mme Agn√®s Firmin Le Bodo\",\n","        \"SPEAKER_02\": \"M. √âric Coquerel\",\n","        \"SPEAKER_03\": \"M. S√©bastien Lecornu\",\n","        \"SPEAKER_04\": \"M. Benjamin Lucas-Lundy\",\n","        \"SPEAKER_05\": \"M. Maxime Michelet\",\n","        \"SPEAKER_06\": \"Mme Marine Le Pen\",\n","        \"SPEAKER_07\": \"M. Jean-Victor Castor\",\n","        \"SPEAKER_08\": \"M. Nicolas Ray\",\n","        \"SPEAKER_09\": \"Inconnu\",\n","        \"SPEAKER_10\": \"Mme la pr√©sidente\",\n","        \"SPEAKER_11\": \"M. Laurent Baumel\",\n","        \"SPEAKER_12\": \"M. Nicolas Metzdorf\",\n","        \"SPEAKER_13\": \"M. Bruno Fuchs\"\n","    }\n","\n","    # 4. LECTURE DU FICHIER DE SAUVEGARDE (Parsing RTTM)\n","    segments_a_traiter = []\n","    with open(RTTM_PATH, 'r') as f:\n","        for line in f:\n","            parts = line.strip().split()\n","            # Format RTTM : SPEAKER file 1 start duration <NA> <NA> name <NA>\n","            if len(parts) >= 8 and parts[0] == \"SPEAKER\":\n","                start = float(parts[3])\n","                duration = float(parts[4])\n","                speaker = parts[7]\n","                end = start + duration\n","                segments_a_traiter.append((start, end, speaker))\n","\n","    print(f\"{len(segments_a_traiter)} segments r√©cup√©r√©s depuis la sauvegarde.\")\n","\n","    # 5. TRANSCRIPTION (Whisper)\n","    print(\"\\nChargement de Whisper et d√©marrage...\")\n","    model = whisper.load_model(\"medium\", device=\"cuda\")\n","    resampler = T.Resample(orig_freq=sample_rate, new_freq=16000).to(\"cuda\")\n","\n","    full_transcript = []\n","\n","    # On utilise la liste charg√©e depuis le fichier texte, plus la variable 'diarization'\n","    for start_sec, end_sec, speaker in segments_a_traiter:\n","        if end_sec - start_sec < 0.5: continue\n","\n","        # D√©coupe\n","        start_f = int(start_sec * sample_rate)\n","        end_f = int(end_sec * sample_rate)\n","        chunk = waveform[:, start_f:end_f]\n","\n","        # Traitement\n","        if chunk.shape[0] > 1: chunk = torch.mean(chunk, dim=0, keepdim=True)\n","        chunk_16k = resampler(chunk)\n","\n","        # Texte\n","        res = model.transcribe(chunk_16k.squeeze().cpu().numpy(), language=\"fr\", fp16=True)\n","        text = res['text'].strip()\n","\n","        if text:\n","            nom = NOMS_REELS.get(speaker, speaker)\n","            ligne = f\"[{start_sec:06.1f}s] {nom} : {text}\"\n","            print(ligne)\n","            full_transcript.append(ligne)\n","\n","    # Sauvegarde\n","    OUTPUT_TXT = \"/content/drive/MyDrive/Qui_parle/TRANSCRIPTION_NOMMEE_REPRISE.txt\"\n","    with open(OUTPUT_TXT, \"w\") as f:\n","        f.write(\"\\n\".join(full_transcript))\n","    print(f\"\\nüéâ Fini ! Sauvegard√© dans {OUTPUT_TXT}\")"],"metadata":{"id":"Jkk-8KZfHnBg"},"execution_count":null,"outputs":[]}]}